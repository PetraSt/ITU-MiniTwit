<<<<<<< HEAD
**Filebeat**

=======
**Author: Petraula Stamou and Dagmara Przygocka**

# ELK
EFK (Elasticsearch, Filebeat, Kibana) is a popular choice for Docker logging due to its various benefits. Firstly, it can handle large amounts of log data generated by Docker containers, making it ideal for applications with high volumes of logs. Secondly, it provides a centralized location for storing and searching log data from multiple containers and hosts, simplifying troubleshooting and system behavior analysis. Additionally, it enables real-time monitoring of logs generated by Docker containers, facilitating timely issue detection and resolution.

Another advantage of EFK is that it provides powerful search and analytics capabilities through Kibana. Users can quickly find and analyze log data, including creating dashboards and alerts. The EFK stack is also highly customizable, enabling users to fine-tune the logging configuration to meet their specific requirements.

Overall, using EFK for Docker logging can significantly improve the manageability and reliability of containerized applications, while providing powerful tools for troubleshooting and analysis.


## Filebeat
>>>>>>> 102b32c80f108dfae9a83c1836f76be3b3648676
Filebeat is a lightweight data shipping agent that is used for forwarding and centralizing log data. It is part of the Elastic Stack, which also includes Elasticsearch, Logstash, and Kibana, and it is designed to work seamlessly with the other components of the stack. Filebeat is used to collect log data from various sources and forward it to a central location, such as Elasticsearch, where it can be indexed and analyzed.

Filebeat supports a variety of input types, including log files, system logs, and application logs, and it can be configured to tail and forward logs in real-time. It also supports a range of output destinations, including Elasticsearch, Logstash, Kafka, and Redis.

Filebeat has a small footprint and low resource utilization, making it ideal for use in distributed architectures where resources are limited. It also provides powerful filtering and processing capabilities, allowing users to selectively collect and transform log data before it is sent to its final destination. Overall, Filebeat is a powerful tool for collecting and forwarding log data and is widely used in large-scale distributed environments to gain insights into system performance, troubleshooting issues, and detecting anomalies.

We configured Filebeat to collect logs from all Docker containers in the /var/lib/docker/containers directory, and to enrich the log events with metadata about the Docker containers they came from using the add_docker_metadata processor. We have also included a decode_json_fields processor to decode any JSON-encoded log messages and store the resulting objects in a new json field.

Additionally, we configured the output destination to send the processed logs to Elasticsearch running at elasticsearch:9200. We have defined three index templates based on the container image names: one for the Filebeat container itself, one for two specific container images (API and APP), and one for an Nginx container.

<<<<<<< HEAD
Finally we enabled JSON logging format for Filebeat logs and disabled metrics logging for Filebeat.
=======
We also enabled JSON logging format for Filebeat logs and disabled metrics logging for Filebeat.

Finally, the following volumes are mounted to the Filebeat container:

**/var/lib/docker:/var/lib/docker:ro** - mounts the Docker host's /var/lib/docker directory to allow Filebeat to collect logs from Docker containers.
**/var/run/docker.sock:/var/run/docker.sock** - mounts the Docker host's Docker socket to allow Filebeat to communicate with the Docker API.
>>>>>>> 102b32c80f108dfae9a83c1836f76be3b3648676

Overall, this configuration will allow us to collect and centralize log data from multiple Docker containers in a structured and organized way, making it easier to analyze and troubleshoot issues in our system.

In the next section we explain the steps in more details.

<<<<<<< HEAD
**Dockerfile**
=======
### Dockerfile
>>>>>>> 102b32c80f108dfae9a83c1836f76be3b3648676

1. We are using the official Docker image for Filebeat version 7.2.0 from Elastic.co.
2. we are using root user to allow copying of the filebeat.yml file to the appropriate directory.
3. Then we copy the filebeat.yml file from the host system to the appropriate directory inside the Docker container.
4. FInally, with CMD ["filebeat", "-e"] command we set the default command to execute when the Docker container starts. In this case, it is the "filebeat" binary with the "-e" flag, which tells Filebeat to log output to the standard error.

<<<<<<< HEAD
**filebeat.yml**

1. filebeat.inputs: This section specifies the input sources for Filebeat to collect logs from.
2. processors: This section specifies the processing steps to apply to the collected logs.
3. output.elasticsearch: This section specifies the output destination for the processed logs.
4. logging.json: true: This setting enables JSON logging format for Filebeat logs.
5. logging.metrics.enabled: false: This setting disables metrics logging for Filebeat.
=======
### filebeat.yml

1. **filebeat.inputs**: This section specifies the input sources for Filebeat to collect logs from.
2. **processors**: This section specifies the processing steps to apply to the collected logs.
3. **output.elasticsearch**: This section specifies the output destination for the processed logs.
4. **logging.json**: true: This setting enables JSON logging format for Filebeat logs.
5. **logging.metrics.enabled**: false: This setting disables metrics logging for Filebeat.


## kibana
Kibana is a data visualization and exploration tool designed to work with Elasticsearch. 

The steps to set it up using the kibana dashboard are:
- create index pattern
    - define index pattern
	![](https://raw.githubusercontent.com/szymongalecki/ITU-MiniTwit/composing/dev_notes/index_pattern.png)
    - configure settings
    ![](https://raw.githubusercontent.com/szymongalecki/ITU-MiniTwit/composing/dev_notes/configure_settings.png)


Next based on the discover tab create visualizations. The team decided to have:
- total number of logs
- logs per api, app in general
- api logs per level
- api logs per request
- logs per day for api
- number of log levels: error, info, debug, warn

In order to have the same visualization set up as the production the following steps have to be taken:
- in production kibana dashboard (url) go to Management tab
- under Kibana section go to Saved Objects
- mark the objects you want to export to your local environment
- export file with .ndjson extension
![](https://raw.githubusercontent.com/szymongalecki/ITU-MiniTwit/composing/dev_notes/export_kibana.png)
- navigate to your local kibana dashboard to Saved Objects and import the file with .ndjson extension
![](https://raw.githubusercontent.com/szymongalecki/ITU-MiniTwit/composing/dev_notes/kibana_import.png)
- go to dashboard or visualization and you will see imported objects, search patterns that you can reuse
![](https://raw.githubusercontent.com/szymongalecki/ITU-MiniTwit/composing/dev_notes/kibana_dashboard.png)
![](https://raw.githubusercontent.com/szymongalecki/ITU-MiniTwit/composing/dev_notes/kibana_discover.png)
![](https://raw.githubusercontent.com/szymongalecki/ITU-MiniTwit/composing/dev_notes/kibana_visualization.png)


## Elasticsearch

Elasticsearch is a distributed search and analytics engine that is part of the Elastic Stack. It is designed to store, search, and analyze large volumes of data in real-time, and is commonly used for log analysis, full-text search, and business intelligence applications.

In our project, Elasticsearch is deployed as a single-node instance using the official Docker image for Elasticsearch version 7.2.0 from Elastic.co.

### Environment Variables

The Elasticsearch container is configured with several environment variables:

**"ES_JAVA_OPTS=-Xms1g -Xmx1g"**: Sets the minimum and maximum heap size for the Elasticsearch JVM to 1 GB. This is a common configuration for running Elasticsearch in a containerized environment.
**"discovery.type=single-node"**: Configures Elasticsearch to run as a single-node cluster. This is appropriate for development and testing environments, but in production, it is recommended to run Elasticsearch as a multi-node cluster.

### Volumes

The Elasticsearch container is configured with a volume:

**"elk-elasticsearch-volume:/usr/share/elasticsearch/data"**: Mounts the "elk-elasticsearch-volume" volume to the "/usr/share/elasticsearch/data" directory inside the Elasticsearch container. This allows Elasticsearch to store its data on the host system, so that it can be persisted across container restarts.


# nginx

Nginx is a high-performance web server that can also be used as a reverse proxy, load balancer, and HTTP cache. In this context, it is being used as a reverse proxy for Elasticsearch and Kibana, with authentication enabled via an .htpasswd file.

In the Dockerfile we specify that the nginx base image should be used and then copies the nginx.conf and .htpasswd files into the image. 
The nginx.conf file specifies two servers: one listening on port 9200 for Elasticsearch, and the other listening on port 5601 for Kibana. Both servers proxy requests to their respective services and authenticate users with the .htpasswd file.

The .htpasswd file contains a single user (the one requested for this assingment and it is the same as prometheus and grafana) with an encrypted password.

Finally, the docker-compose.yml file we specify that ports 9200 and 5601 should be exposed. These ports are then mapped to the same ports on the host machine, allowing you to access Elasticsearch and Kibana through the nginx reverse proxy.

Overall, this setup provides a secure way to access Elasticsearch and Kibana through a single entry point with authentication.
>>>>>>> 102b32c80f108dfae9a83c1836f76be3b3648676
